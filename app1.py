import streamlit as st
import google.generativeai as genai
import json
import os
from pathlib import Path
from dotenv import load_dotenv
from PIL import Image

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© (Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ø£ÙˆÙ„ Ø£Ù…Ø± ÙÙŠ Streamlit ÙˆÙ…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)
st.set_page_config(
    page_title="Skin Care Bot | Ø±ÙˆØªÙŠÙ†Ùƒ Ø§Ù„Ø¬Ù…Ø§Ù„ÙŠ",
    layout="wide",
    page_icon="ğŸŒ¸"
)

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (Caching)
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

@st.cache_resource
def init_model():
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©"""
    try:
        # Ø¨Ù†Ø­Ø§ÙˆÙ„ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙÙ„Ø§Ø´ Ù„Ø£Ù†Ù‡ Ø§Ù„Ø£Ø³Ø±Ø¹ Ù„Ù„ØµÙˆØ±
        model = genai.GenerativeModel('gemini-2.5-flash')
        return model, 'gemini-2.5-flash'
    except:
        return genai.GenerativeModel('gemini-pro'), 'gemini-pro'

model, used_model_name = init_model()

# 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª (Caching)
@st.cache_data
def load_products():
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ÙˆØµÙˆÙ„"""
    try:
        path = Path(__file__).parent / "products_db.json"
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return []

products = load_products()

# 4. Ø§Ù„Ù€ CSS (Ø®Ù„ÙŠÙ†Ø§Ù‡ Ø¨Ø±Ù‡ Ø¹Ø´Ø§Ù† Ù…ÙŠØªØ¹Ø¯Ø´ ØªØ­Ù…ÙŠÙ„Ù‡ ÙƒÙ„ Ø´ÙˆÙŠØ©)
st.markdown("""
    <style>
    .stApp { background: linear-gradient(135deg, #fff5f7 0%, #ffffff 100%); }
    h1 { color: #ff4b6e !important; text-align: center; font-weight: 700; text-shadow: 1px 1px 2px #ffb6c1; }
    section[data-testid="stSidebar"] { background-color: #ffe4e8 !important; }
    .stButton>button { width: 100%; background-color: #ff4b6e !important; color: white !important; border-radius: 25px; }
    .stChatMessage { border-radius: 20px !important; }
    [data-testid="stChatMessageAssistant"] { background-color: #fff0f3 !important; }
    </style>
    """, unsafe_allow_html=True)

# 5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.markdown('<img src="https://cdn-icons-png.flaticon.com/512/3515/3515155.png" style="display: block; margin: auto; width: 80px;">', unsafe_allow_html=True)
st.title("ğŸŒ¸ Skin Care Bot - Ø±ÙÙŠÙ‚ØªÙƒ Ù„Ù„Ø¬Ù…Ø§Ù„ ğŸŒ¸")
st.caption(f"ğŸ”§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù†Ø´Ø·: {used_model_name}")

with st.sidebar:
    st.markdown("### âœ¨ ÙƒÙˆÙ†ÙŠ Ø¬Ù…ÙŠÙ„Ø©ØŒ ÙƒÙˆÙ†ÙŠ Ø£Ù†ØªÙ")
    uploaded_file = st.file_uploader("ğŸ“¸ ØµÙˆØ±ÙŠ Ø¨Ø´Ø±ØªÙƒ Ù„Ù„ØªØ­Ù„ÙŠÙ„", type=['jpg', 'jpeg', 'png'])
    if st.button("ØªÙØ±ÙŠØº Ø§Ù„Ø´Ø§Øª"):
        st.session_state.messages = []
        st.rerun()

# 6. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± (ØªØ¹Ø¯ÙŠÙ„ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø³Ø±Ø¹Ø©)
if uploaded_file is not None:
    image = Image.open(uploaded_file)
    # ØªØµØºÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø±Ø¯
    image.thumbnail((500, 500))
    st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", width=300)
    
    if st.button("ğŸ” Ø­Ù„Ù„ ØµÙˆØ±ØªÙŠ"):
        with st.spinner("Ø«ÙˆØ§Ù†ÙŠ ÙŠØ§ Ø¬Ù…ÙŠÙ„Ø©.. Ø¨Ù†ÙØ­Øµ Ø§Ù„ØµÙˆØ±Ø©..."):
            products_text = json.dumps(products, ensure_ascii=False)
            img_prompt = f"Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¬Ù„Ø¯ÙŠØ© Ù…ØµØ±ÙŠ. Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙˆØ§Ù‚ØªØ±Ø­ Ø±ÙˆØªÙŠÙ† Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙÙ‚Ø·: {products_text}. Ø§ØªÙƒÙ„Ù… Ø¨Ù„Ù‡Ø¬Ø© Ù…ØµØ±ÙŠØ©."
            
            try:
                # Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù„ÙŠ Ø§ØªØ¹Ø±Ù ÙÙˆÙ‚
                response = model.generate_content([img_prompt, image])
                st.info(response.text)
            except Exception as e:
                st.error("Ø§Ù„Ø³ÙŠØ±ÙØ± Ù…Ø¶ØºÙˆØ· Ø´ÙˆÙŠØ©ØŒ Ø¬Ø±Ø¨ÙŠ ØªØ¯ÙˆØ³ÙŠ ØªØ§Ù†ÙŠ.")

# 7. Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Øª (Ø¨Ø³ÙŠØ· ÙˆØ³Ø±ÙŠØ¹)
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ø¨Ø´Ø±ØªÙƒ Ù…Ø­ØªØ§Ø¬Ø© Ø¥ÙŠÙ‡ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ø¨ÙÙƒØ±..."):
            # Ø¯Ù…Ø¬ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ù…Ø¨Ø¹ÙˆØªØ©
            full_prompt = f"Ø£Ù†Øª Ø®Ø¨ÙŠØ±Ø© ØªØ¬Ù…ÙŠÙ„ Ù…ØµØ±ÙŠØ©. Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©: {json.dumps(products)}. Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
            response = model.generate_content(full_prompt)
            st.markdown(response.text)
            st.session_state.messages.append({"role": "assistant", "content": response.text})
