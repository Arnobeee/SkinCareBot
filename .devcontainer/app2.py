import streamlit as st
import google.generativeai as genai
import json
import os
from pathlib import Path
from dotenv import load_dotenv
from PIL import Image
import io

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Skin Care Bot | Ø±ÙˆØªÙŠÙ†Ùƒ Ø§Ù„Ø¬Ù…Ø§Ù„ÙŠ",
    layout="wide",
    page_icon="ğŸŒ¸"
)

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    st.error("âš ï¸ Ø®Ø·Ø£: Ù…ÙÙŠØ´ API Key! Ø§ØªØ£ÙƒØ¯ Ø¥Ù† Ù…Ù„Ù .env Ø´ØºØ§Ù„ ÙˆÙÙŠÙ‡ Ø§Ù„Ù€ GOOGLE_API_KEY")
    st.stop()

genai.configure(api_key=api_key)

@st.cache_resource
def init_model():
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø¹ ØªÙØ¹ÙŠÙ„ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø¥ØµØ¯Ø§Ø± 2.5"""
    try:
        # Ø¨Ù†Ø­Ø¯Ø¯ Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ± Ø¹Ø´Ø§Ù† Ù†Ù…Ù†Ø¹ Ø§Ù„Ù€ UI Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚
        model = genai.GenerativeModel(
            model_name='gemini-2.5-flash',
            generation_config={
                "temperature": 0.7,
                "top_p": 0.95,
                "max_output_tokens": 2048,
            }
        )
        return model, 'gemini-2.5-flash'
    except Exception as e:
        st.warning(f"ÙØ´Ù„ ØªØ´ØºÙŠÙ„ 2.5ØŒ Ø¨Ù†Ø¬Ø±Ø¨ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø©: {str(e)}")
        return genai.GenerativeModel('gemini-1.5-flash'), 'gemini-1.5-flash'

model, used_model_name = init_model()

# 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
@st.cache_data
def load_products():
    try:
        path = Path(__file__).parent / "products_db.json"
        if path.exists():
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        return [{"name": "Generic Moisturizer", "type": "cream"}] # Ø¯Ø§ØªØ§ Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
    except:
        return []

products = load_products()

# 4. Ø§Ù„Ù€ CSS
st.markdown("""
    <style>
    .stApp { background: linear-gradient(135deg, #fff5f7 0%, #ffffff 100%); }
    h1 { color: #ff4b6e !important; text-align: center; font-weight: 700; }
    .stSpinner > div > div { border-top-color: #ff4b6e !important; }
    </style>
    """, unsafe_allow_html=True)

# 5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.title("ğŸŒ¸ Skin Care Bot - Ø±ÙÙŠÙ‚ØªÙƒ Ù„Ù„Ø¬Ù…Ø§Ù„ ğŸŒ¸")
st.caption(f"ğŸ”§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù†Ø´Ø· Ø­Ø§Ù„ÙŠØ§Ù‹: {used_model_name}")

with st.sidebar:
    st.markdown("### âœ¨ ÙƒÙˆÙ†ÙŠ Ø¬Ù…ÙŠÙ„Ø©ØŒ ÙƒÙˆÙ†ÙŠ Ø£Ù†ØªÙ")
    uploaded_file = st.file_uploader("ğŸ“¸ ØµÙˆØ±ÙŠ Ø¨Ø´Ø±ØªÙƒ Ù„Ù„ØªØ­Ù„ÙŠÙ„", type=['jpg', 'jpeg', 'png'])
    if st.button("ØªÙØ±ÙŠØº Ø§Ù„Ø´Ø§Øª"):
        st.session_state.messages = []
        st.rerun()

# 6. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± (ØªØ¹Ø¯ÙŠÙ„ Ø¬ÙˆÙ‡Ø±ÙŠ Ù„ÙÙƒ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø©)
if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", width=300)
    
    if st.button("ğŸ” Ø­Ù„Ù„ ØµÙˆØ±ØªÙŠ"):
        with st.spinner("Ø«ÙˆØ§Ù†ÙŠ ÙŠØ§ Ø¬Ù…ÙŠÙ„Ø©.. Gemini 2.5 Ø¨ÙŠÙÙƒØ± ÙÙŠ Ø­Ø§Ù„ØªÙƒ..."):
            try:
                products_text = json.dumps(products, ensure_ascii=False)
                # Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…Ø·ÙˆØ± Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø¨Ø¯
                img_prompt = (
                    f"Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¬Ù„Ø¯ÙŠØ© Ù…ØµØ±ÙŠ Ù…Ø­ØªØ±Ù. Ø­Ù„Ù„ Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ø´Ø±Ø© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¯Ù‚Ø©. "
                    f"Ø§Ù‚ØªØ±Ø­ Ø±ÙˆØªÙŠÙ† Ù…Ø®ØµØµ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙÙ‚Ø·: {products_text}. "
                    "Ø§ØªÙƒÙ„Ù… Ø¨Ù„Ù‡Ø¬Ø© Ù…ØµØ±ÙŠØ© ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ©."
                )
                
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù„Ù€ Gemini 2.5
                response = model.generate_content([img_prompt, image])
                
                st.success("âœ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¬Ø§Ù‡Ø²:")
                st.write(response.text)
            except Exception as e:
                st.error(f"Ø­ØµÙ„Øª Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")

# 7. Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Øª
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ø¨Ø´Ø±ØªÙƒ Ù…Ø­ØªØ§Ø¬Ø© Ø¥ÙŠÙ‡ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ø¨Ø±Ø§Ø¬Ø¹ Ø¯Ø§ØªØ§ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙˆØ¨ÙÙƒØ± Ù„Ùƒ..."):
            try:
                full_prompt = f"Ø£Ù†Øª Ø®Ø¨ÙŠØ±Ø© ØªØ¬Ù…ÙŠÙ„ Ù…ØµØ±ÙŠØ© Ø´Ø§Ø·Ø±Ø©. Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ø¹Ù†Ø¯Ù†Ø§: {json.dumps(products)}. Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {prompt}"
                response = model.generate_content(full_prompt)
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error("Ø§Ù„Ø³ÙŠØ³ØªÙ… ÙˆÙ‚Ø¹ Ù…Ù†ÙŠ Ø«Ø§Ù†ÙŠØ©ØŒ Ø¬Ø±Ø¨ÙŠ ØªØ³Ø£Ù„ÙŠ ØªØ§Ù†ÙŠ.")
